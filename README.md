# ğŸ”‹ Battery State of Health (SOH) Prediction using Pyramid TCN-Transformer

Deep Learning model for predicting lithium-ion battery health in electric vehicles using hybrid Temporal Convolutional Network (TCN) and Transformer architecture.

---

## ğŸ“Œ Project Overview

This project predicts **State of Health (SOH)** of lithium-ion batteries using a novel **Pyramid TCN-Transformer** deep learning model. The model achieves high accuracy by combining:
- **Temporal Convolutional Networks (TCN)** for multi-scale feature extraction
- **Transformer** for long-range dependency modeling
- **Attention mechanisms** for adaptive feature weighting

---

## ğŸ¯ Why This Project?

Battery health estimation is crucial for:
- âœ… **Safety**: Prevent unexpected battery failures
- âœ… **Reliability**: Accurate range prediction for EVs
- âœ… **Maintenance**: Optimize battery replacement timing
- âœ… **Cost**: Reduce warranty expenses

---

## ğŸ—‚ï¸ Dataset

**CALCE Battery Dataset** (Center for Advanced Life Cycle Engineering)
- **Batteries**: CS2_35, CS2_36, CS2_37, CS2_38
- **Features**: Voltage, Current, Temperature, Capacity
- **Drive Cycles**: US06, FUDS, DST
- **Source**: [CALCE Battery Research Group](https://web.calce.umd.edu/batteries/data.htm)

---

## ğŸ—ï¸ Model Architecture

### Pyramid TCN-Transformer

```
Input (5 features) 
    â†“
TCN (3 parallel branches with dilations: 1, 2, 4)
    â†“
Concatenate features
    â†“
Transformer Encoder (2 layers, 8 attention heads)
    â†“
Attention Pooling
    â†“
MLP Regression Head
    â†“
SOH Prediction
```

### Key Components:

1. **TCN Block**:
   - 3 parallel branches with dilations [1, 2, 4]
   - Captures multi-scale temporal patterns
   - 24 channels per branch

2. **Transformer Encoder**:
   - 2 layers with 8 attention heads
   - Hidden dimension: 72
   - Models long-range dependencies

3. **Attention Pooling**:
   - Learns to weight important time steps
   - Adaptive feature selection

4. **MLP Head**:
   - Final regression layer
   - Outputs SOH percentage

---

## ğŸ“Š Model Performance

**On CALCE Dataset:**
- **MAPE**: 3.21% (Mean Absolute Percentage Error)
- **RÂ²**: 0.992 (Coefficient of Determination)
- **MAE**: 0.0129 (Mean Absolute Error)
- **RMSE**: 0.0169 (Root Mean Square Error)

**Computational Efficiency:**
- **Parameters**: 137.8K
- **Training Time**: ~18 minutes (20 epochs on RTX 3080)
- **Inference Time**: 2.1ms per sample (GPU)
- **GPU Memory**: 850 MB

---

## ğŸ’» Technologies Used

- **Python** 3.8+
- **PyTorch** 2.0+ (Deep Learning Framework)
- **PyTorch Lightning** (Training Pipeline)
- **NumPy** (Numerical Computing)
- **Pandas** (Data Processing)
- **Matplotlib** (Visualization)
- **Scikit-learn** (Metrics & Preprocessing)
- **Google Colab** (Development Environment)

---

## ğŸš€ How to Use

### 1. Setup Environment

```bash
# Install required packages
pip install torch pytorch-lightning numpy pandas matplotlib scikit-learn
```

### 2. Prepare Dataset

1. Download CALCE dataset from [here](https://web.calce.umd.edu/batteries/data.htm)
2. Extract files and organize:
   ```
   Dataset/
   â”œâ”€â”€ CS2_35/
   â”œâ”€â”€ CS2_36/
   â”œâ”€â”€ CS2_37/
   â””â”€â”€ CS2_38/
   ```

### 3. Run the Notebook

1. Open `TCN-Transformer.ipynb` in Google Colab or Jupyter
2. Mount Google Drive (if using Colab)
3. Update the dataset path:
   ```python
   dir_path = "/path/to/your/Dataset/"
   ```
4. Run all cells sequentially

### 4. Model Training

The notebook will:
- Load and preprocess CALCE data
- Build train/validation/test splits
- Train the TCN-Transformer model
- Evaluate performance metrics
- Generate prediction plots

### 5. Key Hyperparameters

```python
n_features = 5          # Input features
hidden_dim = 72         # Transformer hidden dimension
n_dilations = [1, 2, 4] # TCN dilation rates
nhead = 8               # Transformer attention heads
lr = 1e-4               # Learning rate
max_epochs = 80         # Maximum training epochs
batch_size = 32         # Training batch size
window_size = 8         # Sequence length
```

---

## ğŸ“ Repository Structure

```
battery-soh-prediction/
â”‚
â”œâ”€â”€ TCN-Transformer.ipynb   # Main notebook (Pyramid TCN-Transformer)
â”œâ”€â”€ LSTM.ipynb                      # LSTM baseline
â”œâ”€â”€ CNN_LSTM.ipynb                  # CNN+LSTM baseline
â”œâ”€â”€ Transformer.ipynb               # Transformer baseline
â”œâ”€â”€ TCN_-1__Transformer.ipynb       # TCN variant 1
â”œâ”€â”€ TCN_-2__Transformer.ipynb       # TCN variant 2
â”œâ”€â”€ README.md                       # This file
â””â”€â”€ requirements.txt                # Python dependencies
```

---

## ğŸ“ˆ Results Visualization

The model generates:
- **SOH prediction curves** (Actual vs Predicted)
- **Error distribution plots**
- **Training/validation loss curves**
- **Performance comparison charts**

---

## ğŸ“ Key Features

âœ… **Multi-scale temporal modeling** with Pyramid TCN  
âœ… **Long-range dependency** capture with Transformer  
âœ… **Attention-based feature selection**  
âœ… **Real-time prediction** capability  
âœ… **High accuracy** on standard battery datasets  
âœ… **Efficient architecture** with only 137K parameters  

---

## ğŸ”§ Model Components Explained

### Why Pyramid TCN?
- Captures patterns at multiple time scales
- Parallel processing (faster than RNN/LSTM)
- No vanishing gradient problems
- Flexible receptive fields

### Why Transformer?
- Models long-term dependencies effectively
- Self-attention captures global context
- Parallel computation advantage
- Better than sequential LSTM/GRU

### Why Attention Pooling?
- Learns which time steps are important
- Adaptive weighting of features
- Improves prediction accuracy

---

## ğŸ“š References

### Dataset
- [CALCE Battery Research Group](https://web.calce.umd.edu/batteries/data.htm)

### Key Papers
- **TCN**: Bai et al., "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling"
- **Transformer**: Vaswani et al., "Attention Is All You Need"
- Various battery SOH estimation research papers


## ğŸ”® Future Improvements

- [ ] Add more battery datasets (NASA, MIT)
- [ ] Implement online learning
- [ ] Deploy as REST API
- [ ] Add model explainability (SHAP, Attention visualization)
- [ ] Extend to multi-task learning (SOC + SOH)
- [ ] Mobile deployment (PyTorch Mobile)

---
